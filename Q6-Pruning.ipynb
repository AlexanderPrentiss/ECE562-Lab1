{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SblI4JHed_gb"
   },
   "outputs": [],
   "source": [
    "# # this mounts your Google Drive to the Colab VM.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# # enter the foldername in your Drive where you have saved the unzipped\n",
    "# # assignment folder, e.g. 'ece697ls/assignments/assignment3/'\n",
    "# FOLDERNAME = None\n",
    "# assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# # now that we've mounted your Drive, this ensures that\n",
    "# # the Python interpreter of the Colab VM can load\n",
    "# # python files from within it.\n",
    "# import sys\n",
    "# sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "# %cd /content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDCKl_T8d_gg",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Pruning\n",
    "\n",
    "After learning, neural networks have modified and learned a set of parameters to perform our classification task. However, such parameters are costly to maintain and do not hold the same importance.\n",
    "\n",
    "Wouldn't it be great could optimize our resource usage by dropping less important values ? This is where pruning comes into play.\n",
    "\n",
    "Pruning is a technique that cuts off parameters/structures from a model to increase sparcity and decrease overall model size, similar to cutting leafs or branches from bushes and trees. This process can lead to smaller memory consumption with minimal accuracy reduction. Moreover, pruning the network may also provide a speedup since there will be less operations being performed.\n",
    "\n",
    "The pruning process can be performed during the end of an epoch of training or after training is complete. Experimenting to find out which way works the best is part of the fun !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "P242B5PEtU6N"
   },
   "outputs": [],
   "source": [
    "from ece662.pruning_helper import test_model, load_model\n",
    "from ece662.data_utils import get_CINIC10_data\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76IT2Z-P7aYm"
   },
   "source": [
    "Below we will load a pre-trained model for you to work on. If you prefer, you can save your own model from the previous Tensorflow/Pytorch task and load it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKM5JRLtuVbJ"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/alex/Developer/School/ECE-562/assignment1_colab/assignment1/ece662/models/torch.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m test_data = [data[\u001b[33m'\u001b[39m\u001b[33mX_test\u001b[39m\u001b[33m'\u001b[39m],data[\u001b[33m'\u001b[39m\u001b[33my_test\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m      8\u001b[39m path = os.path.join(\u001b[33m'\u001b[39m\u001b[33m/home/alex/Developer/School/ECE-562/assignment1_colab/assignment1\u001b[39m\u001b[33m'\u001b[39m.format(\u001b[38;5;28;01mNone\u001b[39;00m), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mece662/models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m model = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m test_model(model,test_data,mode=mode)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/School/ECE-562/assignment1_colab/assignment1/ece662/pruning_helper.py:71\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(path, num_classes, mode)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m(mode==\u001b[33m'\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     70\u001b[39m   model = ConvNet(\u001b[32m6\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m   model.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m(mode==\u001b[33m'\u001b[39m\u001b[33mtensorflow\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     73\u001b[39m   model = keras.models.load_model(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ECE562-3-11/lib/python3.11/site-packages/torch/serialization.py:1484\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1482\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1486\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1487\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1488\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1489\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ECE562-3-11/lib/python3.11/site-packages/torch/serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/ECE562-3-11/lib/python3.11/site-packages/torch/serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/alex/Developer/School/ECE-562/assignment1_colab/assignment1/ece662/models/torch.model'"
     ]
    }
   ],
   "source": [
    "#This code may take a while to execute as it is training a network form scratch\n",
    "\n",
    "data = get_CINIC10_data()\n",
    "mode = 'torch'#torch or tensorflow\n",
    "\n",
    "test_data = [data['X_test'],data['y_test']]\n",
    "\n",
    "path = \"/Users/alexprentiss/Developer/School/ECE-562/assignment1_colab/assignment1/ece662/models/torch.model\"\n",
    "\n",
    "model = load_model(path,mode=mode)\n",
    "test_model(model,test_data,mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZOsPryFd_gi",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## Unstructured Pruning\n",
    "\n",
    "Unstructured Pruning is usually related to the pruning of weights in neural networks. The general idea is to select a set of weights according to a policy and setting them up to zero. \n",
    "\n",
    "Common policies are random weight selection or selecting the smallers weights. \n",
    "Unstructured Pruning can be performed in one or multiple layers within the same network.\n",
    "\n",
    "Altough in theory Unstructured Pruning should decrease the number of operations performed during execution there should be explicit support within the framework or hardware to bypass such operations, otherwise it will just operated over zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjxdSF2xmCei"
   },
   "source": [
    "### Perform Pruning\n",
    "\n",
    "Using the model trained in the previous step using pytorch, perform unstructured pruning in the weights of the model by removing x% of the smallest weights. \n",
    "\n",
    "*   Increment global pruning by 10% until reaching total of 80% pruned weights\n",
    "*   Perform inference at the end of each pruning and observe the impact into the accuracy.\n",
    "\n",
    "\n",
    "Note: The percentages are related to the entire model, not per layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KaGWTxBcmkzc"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO: Perform unstructured Pruning over the trained model using 3 different  \n",
    "# prunning percentages.                                \n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zH6e3pCMm2in"
   },
   "source": [
    "## Inline Question 1:\n",
    "\n",
    "What happened with the accuracy as the % of pruning increased ?\n",
    "Why was that the case?\n",
    "\n",
    "\n",
    "## Answer: \n",
    "\n",
    "[FILL YOU ANSWER HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JN5CMlgUd_gj",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## Structured Pruning\n",
    "\n",
    "Structured Pruning consists of removing a bigger chunk of the network parameters at the same time. Instead of removing only a few weights, it is commonplace to remove entire neurons. \n",
    "\n",
    "For example, in Convolutional Layers, removing filters can be beneficial to improve performance as it greatly decreases the amount of computation performed. However, some of these changes may affect output dimensions which may be carried over to other parts of the network. Therefore, when performing structured pruning one must always be aware of which parameters are going to be affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAr9SLo25Gyx"
   },
   "source": [
    "Using the previously trained model in the CINIC-10, perform Structured Prunning only in the Convolution layers of the DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JFPiHtBohTus"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO: Perform unstrucuted Pruning over the trained model using 3 different  \n",
    "# prunning percentages.                                \n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h85dg7kVd_gk"
   },
   "source": [
    "## Inline Question 2:\n",
    "\n",
    "What is the difference between performing Structured Pruning vs Dropout ? \n",
    "Why would it be beneficial to perform both techniques when developing a Neural Network?\n",
    "\n",
    "\n",
    "## Answer: \n",
    "\n",
    "[FILL YOU ANSWER HERE]\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "ECE562-3-11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
